(window["webpackJsonp"] = window["webpackJsonp"] || []).push([[77],{

/***/ "./node_modules/@codemirror/legacy-modes/mode/sass.js":
/*!************************************************************!*\
  !*** ./node_modules/@codemirror/legacy-modes/mode/sass.js ***!
  \************************************************************/
/*! exports provided: sass */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, \"sass\", function() { return sass; });\n/* harmony import */ var _css_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./css.js */ \"./node_modules/@codemirror/legacy-modes/mode/css.js\");\n\nconst propertyKeywords = new Set(_css_js__WEBPACK_IMPORTED_MODULE_0__[\"keywords\"].properties)\nconst colorKeywords = new Set(_css_js__WEBPACK_IMPORTED_MODULE_0__[\"keywords\"].colors)\nconst valueKeywords = new Set(_css_js__WEBPACK_IMPORTED_MODULE_0__[\"keywords\"].values)\nconst fontProperties = new Set(_css_js__WEBPACK_IMPORTED_MODULE_0__[\"keywords\"].fonts)\n\nfunction tokenRegexp(words) {\n  return new RegExp(\"^\" + words.join(\"|\"))\n}\n\nlet keywords = [\"true\", \"false\", \"null\", \"auto\"]\nlet keywordsRegexp = new RegExp(\"^\" + keywords.join(\"|\"))\n\nlet operators = [\"\\\\(\", \"\\\\)\", \"=\", \">\", \"<\", \"==\", \">=\", \"<=\", \"\\\\+\", \"-\",\n                 \"\\\\!=\", \"/\", \"\\\\*\", \"%\", \"and\", \"or\", \"not\", \";\",\"\\\\{\",\"\\\\}\",\":\"]\nlet opRegexp = tokenRegexp(operators)\n\nlet pseudoElementsRegexp = /^::?[a-zA-Z_][\\w\\-]*/\n\nlet word\n\nfunction isEndLine(stream) {\n  return !stream.peek() || stream.match(/\\s+$/, false)\n}\n\nfunction urlTokens(stream, state) {\n  let ch = stream.peek()\n\n  if (ch === \")\") {\n    stream.next()\n    state.tokenizer = tokenBase\n    return \"operator\"\n  } else if (ch === \"(\") {\n    stream.next()\n    stream.eatSpace()\n\n    return \"operator\"\n  } else if (ch === \"'\" || ch === '\"') {\n    state.tokenizer = buildStringTokenizer(stream.next())\n    return \"string\"\n  } else {\n    state.tokenizer = buildStringTokenizer(\")\", false)\n    return \"string\"\n  }\n}\nfunction comment(indentation, multiLine) {\n  return function(stream, state) {\n    if (stream.sol() && stream.indentation() <= indentation) {\n      state.tokenizer = tokenBase\n      return tokenBase(stream, state)\n    }\n\n    if (multiLine && stream.skipTo(\"*/\")) {\n      stream.next()\n      stream.next()\n      state.tokenizer = tokenBase\n    } else {\n      stream.skipToEnd()\n    }\n\n    return \"comment\"\n  }\n}\n\nfunction buildStringTokenizer(quote, greedy) {\n  if (greedy == null) { greedy = true }\n\n  function stringTokenizer(stream, state) {\n    let nextChar = stream.next()\n    let peekChar = stream.peek()\n    let previousChar = stream.string.charAt(stream.pos-2)\n\n    let endingString = ((nextChar !== \"\\\\\" && peekChar === quote) || (nextChar === quote && previousChar !== \"\\\\\"))\n\n    if (endingString) {\n      if (nextChar !== quote && greedy) { stream.next() }\n      if (isEndLine(stream)) {\n        state.cursorHalf = 0\n      }\n      state.tokenizer = tokenBase\n      return \"string\"\n    } else if (nextChar === \"#\" && peekChar === \"{\") {\n      state.tokenizer = buildInterpolationTokenizer(stringTokenizer)\n      stream.next()\n      return \"operator\"\n    } else {\n      return \"string\"\n    }\n  }\n\n  return stringTokenizer\n}\n\nfunction buildInterpolationTokenizer(currentTokenizer) {\n  return function(stream, state) {\n    if (stream.peek() === \"}\") {\n      stream.next()\n      state.tokenizer = currentTokenizer\n      return \"operator\"\n    } else {\n      return tokenBase(stream, state)\n    }\n  }\n}\n\nfunction indent(state, stream) {\n  if (state.indentCount == 0) {\n    state.indentCount++\n    let lastScopeOffset = state.scopes[0].offset\n    let currentOffset = lastScopeOffset + stream.indentUnit\n    state.scopes.unshift({ offset:currentOffset })\n  }\n}\n\nfunction dedent(state) {\n  if (state.scopes.length == 1) return\n\n  state.scopes.shift()\n}\n\nfunction tokenBase(stream, state) {\n  let ch = stream.peek()\n\n  // Comment\n  if (stream.match(\"/*\")) {\n    state.tokenizer = comment(stream.indentation(), true)\n    return state.tokenizer(stream, state)\n  }\n  if (stream.match(\"//\")) {\n    state.tokenizer = comment(stream.indentation(), false)\n    return state.tokenizer(stream, state)\n  }\n\n  // Interpolation\n  if (stream.match(\"#{\")) {\n    state.tokenizer = buildInterpolationTokenizer(tokenBase)\n    return \"operator\"\n  }\n\n  // Strings\n  if (ch === '\"' || ch === \"'\") {\n    stream.next()\n    state.tokenizer = buildStringTokenizer(ch)\n    return \"string\"\n  }\n\n  if (!state.cursorHalf) {\n    // first half i.e. before : for key-value pairs\n    // including selectors\n    if (ch === \"-\") {\n      if (stream.match(/^-\\w+-/)) {\n        return \"meta\"\n      }\n    }\n\n    if (ch === \".\") {\n      stream.next()\n      if (stream.match(/^[\\w-]+/)) {\n        indent(state, stream)\n        return \"qualifier\"\n      } else if (stream.peek() === \"#\") {\n        indent(state, stream)\n        return \"tag\"\n      }\n    }\n\n    if (ch === \"#\") {\n      stream.next()\n      // ID selectors\n      if (stream.match(/^[\\w-]+/)) {\n        indent(state, stream)\n        return \"builtin\"\n      }\n      if (stream.peek() === \"#\") {\n        indent(state, stream)\n        return \"tag\"\n      }\n    }\n\n    // Variables\n    if (ch === \"$\") {\n      stream.next()\n      stream.eatWhile(/[\\w-]/)\n      return \"variable-2\"\n    }\n\n    // Numbers\n    if (stream.match(/^-?[0-9\\.]+/))\n      return \"number\"\n\n    // Units\n    if (stream.match(/^(px|em|in)\\b/))\n      return \"unit\"\n\n    if (stream.match(keywordsRegexp))\n      return \"keyword\"\n\n    if (stream.match(/^url/) && stream.peek() === \"(\") {\n      state.tokenizer = urlTokens\n      return \"atom\"\n    }\n\n    if (ch === \"=\") {\n      // Match shortcut mixin definition\n      if (stream.match(/^=[\\w-]+/)) {\n        indent(state, stream)\n        return \"meta\"\n      }\n    }\n\n    if (ch === \"+\") {\n      // Match shortcut mixin definition\n      if (stream.match(/^\\+[\\w-]+/)) {\n        return \"meta\"\n      }\n    }\n\n    if (ch === \"@\") {\n      if (stream.match('@extend')) {\n        if (!stream.match(/\\s*[\\w]/))\n          dedent(state)\n      }\n    }\n\n\n    // Indent Directives\n    if (stream.match(/^@(else if|if|media|else|for|each|while|mixin|function)/)) {\n      indent(state, stream)\n      return \"def\"\n    }\n\n    // Other Directives\n    if (ch === \"@\") {\n      stream.next()\n      stream.eatWhile(/[\\w-]/)\n      return \"def\"\n    }\n\n    if (stream.eatWhile(/[\\w-]/)) {\n      if (stream.match(/ *: *[\\w-\\+\\$#!\\(\"']/,false)) {\n        word = stream.current().toLowerCase()\n        let prop = state.prevProp + \"-\" + word\n        if (propertyKeywords.has(prop)) {\n          return \"property\"\n        } else if (propertyKeywords.has(word)) {\n          state.prevProp = word\n          return \"property\"\n        } else if (fontProperties.has(word)) {\n          return \"property\"\n        }\n        return \"tag\"\n      } else if (stream.match(/ *:/,false)) {\n        indent(state, stream)\n        state.cursorHalf = 1\n        state.prevProp = stream.current().toLowerCase()\n        return \"property\"\n      } else if (stream.match(/ *,/,false)) {\n        return \"tag\"\n      } else {\n        indent(state, stream)\n        return \"tag\"\n      }\n    }\n\n    if (ch === \":\") {\n      if (stream.match(pseudoElementsRegexp)) { // could be a pseudo-element\n        return \"type\"\n      }\n      stream.next()\n      state.cursorHalf=1\n      return \"operator\"\n    }\n  } else {\n    if (ch === \"#\") {\n      stream.next()\n      // Hex numbers\n      if (stream.match(/[0-9a-fA-F]{6}|[0-9a-fA-F]{3}/)) {\n        if (isEndLine(stream)) {\n          state.cursorHalf = 0\n        }\n        return \"number\"\n      }\n    }\n\n    // Numbers\n    if (stream.match(/^-?[0-9\\.]+/)) {\n      if (isEndLine(stream)) {\n        state.cursorHalf = 0\n      }\n      return \"number\"\n    }\n\n    // Units\n    if (stream.match(/^(px|em|in)\\b/)) {\n      if (isEndLine(stream)) {\n        state.cursorHalf = 0\n      }\n      return \"unit\"\n    }\n\n    if (stream.match(keywordsRegexp)) {\n      if (isEndLine(stream)) {\n        state.cursorHalf = 0\n      }\n      return \"keyword\"\n    }\n\n    if (stream.match(/^url/) && stream.peek() === \"(\") {\n      state.tokenizer = urlTokens\n      if (isEndLine(stream)) {\n        state.cursorHalf = 0\n      }\n      return \"atom\"\n    }\n\n    // Variables\n    if (ch === \"$\") {\n      stream.next()\n      stream.eatWhile(/[\\w-]/)\n      if (isEndLine(stream)) {\n        state.cursorHalf = 0\n      }\n      return \"variable-2\"\n    }\n\n    // bang character for !important, !default, etc.\n    if (ch === \"!\") {\n      stream.next()\n      state.cursorHalf = 0\n      return stream.match(/^[\\w]+/) ? \"keyword\": \"operator\"\n    }\n\n    if (stream.match(opRegexp)) {\n      if (isEndLine(stream)) {\n        state.cursorHalf = 0\n      }\n      return \"operator\"\n    }\n\n    // attributes\n    if (stream.eatWhile(/[\\w-]/)) {\n      if (isEndLine(stream)) {\n        state.cursorHalf = 0\n      }\n      word = stream.current().toLowerCase()\n      if (valueKeywords.has(word)) {\n        return \"atom\"\n      } else if (colorKeywords.has(word)) {\n        return \"keyword\"\n      } else if (propertyKeywords.has(word)) {\n        state.prevProp = stream.current().toLowerCase()\n        return \"property\"\n      } else {\n        return \"tag\"\n      }\n    }\n\n    if (isEndLine(stream)) {\n      state.cursorHalf = 0\n      return null\n    }\n  }\n\n  if (stream.match(opRegexp))\n    return \"operator\"\n\n  stream.next()\n  return null\n}\n\nfunction tokenLexer(stream, state) {\n  if (stream.sol()) state.indentCount = 0\n  let style = state.tokenizer(stream, state)\n  let current = stream.current()\n\n  if (current === \"@return\" || current === \"}\") {\n    dedent(state)\n  }\n\n  if (style !== null) {\n    let startOfToken = stream.pos - current.length\n\n    let withCurrentIndent = startOfToken + (stream.indentUnit * state.indentCount)\n\n    let newScopes = []\n\n    for (let i = 0; i < state.scopes.length; i++) {\n      let scope = state.scopes[i]\n\n      if (scope.offset <= withCurrentIndent)\n        newScopes.push(scope)\n    }\n\n    state.scopes = newScopes\n  }\n\n  return style\n}\n\nconst sass = {\n  startState: function() {\n    return {\n      tokenizer: tokenBase,\n      scopes: [{offset: 0, type: \"sass\"}],\n      indentCount: 0,\n      cursorHalf: 0,  // cursor half tells us if cursor lies after (1)\n      // or before (0) colon (well... more or less)\n      definedVars: [],\n      definedMixins: []\n    }\n  },\n  token: function(stream, state) {\n    let style = tokenLexer(stream, state)\n    state.lastToken = { style: style, content: stream.current() }\n    return style\n  },\n\n  indent: function(state) {\n    return state.scopes[0].offset\n  },\n\n  languageData: {\n    commentTokens: {line: \"//\", block: {open: \"/*\", close: \"*/\"}},\n    autocomplete: _css_js__WEBPACK_IMPORTED_MODULE_0__[\"keywords\"].all\n  }\n}\n\n\n//# sourceURL=webpack:///./node_modules/@codemirror/legacy-modes/mode/sass.js?");

/***/ })

}]);